{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired/based on https://github.com/rayidghani/magicloops and https://github.com/dssg/MLforPublicPolicy/blob/master/labs/2019/lab6_feature_generation_sol.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_evictions as pipeline\n",
    "import ml_loop_evictions as loop\n",
    "\n",
    "import importlib\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "datafile = \"data/tracts.csv\"\n",
    "\n",
    "#Read data, parsing year column to date type\n",
    "data = pd.read_csv(datafile, parse_dates=['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows that do not have eviction-rate. We do not want to impute this value\n",
    "\n",
    "data.dropna(subset=['eviction-rate'], inplace=True)\n",
    "data['eviction-rate'].isnull().values.any()\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create outcome label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eviction_rate(df, year,geoid):\n",
    "    \n",
    "  data_to_return = df.loc[(df['year'] == year) & (df['GEOID'] == geoid)]\n",
    "  \n",
    "  if(data_to_return.empty):\n",
    "    return 0\n",
    "  \n",
    "  return data_to_return['eviction-rate'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "#Obtain eviction-rate cutoff for the top 10%, for each year\n",
    "cutoff_10_percent={}\n",
    "for year in range(2000,2017):\n",
    "    year = pd.Timestamp(year,1,1)\n",
    "    cutoff_10_percent[year]=data.loc[data['year'] == year]['eviction-rate'].quantile(.9)\n",
    "    \n",
    "top_10_eviction_rate_in_any_next_3_years_column = np.zeros(len(data))\n",
    "print(top_10_eviction_rate_in_any_next_3_years_column.shape)\n",
    "\n",
    "#Generate this feature now\n",
    "# top_10_eviction_rate_last_year_column = np.zeros(len(data))\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "  \n",
    "  #Because the outcome will come from eviction-rate in next 3 years and we have data till 2016,\n",
    "  #features data bust be from 2013 or before\n",
    "  \n",
    "  #We will also limit rows so that they all have one year before them (to calculate to_10_previous_year_feature)\n",
    "  # & row['year']>=pd.Timestamp(2001,1,1)\n",
    "  if(row['year']<=pd.Timestamp(2013,1,1)):    \n",
    "    \n",
    "    found_year_where_eviction_was_in_top_10_percent=0\n",
    "    \n",
    "    #Get eviction for the next 3 years\n",
    "    for i in range(1,4):\n",
    "      date_in_i_years = row['year'] + relativedelta(years=i)\n",
    "      eviction_rate_in_i_years = get_eviction_rate(data, date_in_i_years,row['GEOID'])\n",
    "    \n",
    "      top_10_eviction_rate_in_i_years = 1 if eviction_rate_in_i_years>= cutoff_10_percent[date_in_i_years] else 0\n",
    "      \n",
    "\n",
    "      \n",
    "      #If we found one year that meets requirement, we are done with looping\n",
    "      if(top_10_eviction_rate_in_i_years==1):\n",
    "        found_year_where_eviction_was_in_top_10_percent=1\n",
    "        break\n",
    "\n",
    "    top_10_eviction_rate_in_any_next_3_years_column[index]=found_year_where_eviction_was_in_top_10_percent\n",
    "\n",
    "    \n",
    "    #Generating feature now\n",
    "#     date_last_years = row['year'] - relativedelta(years=i)\n",
    "#     eviction_rate_last_year = get_eviction_rate(data, date_last_years,row['GEOID'])\n",
    "#     top_10_eviction_rate_last_year = 1 if eviction_rate_last_year>= cutoff_10_percent[date_last_years] else 0\n",
    "\n",
    "#     top_10_eviction_rate_last_year_column[index] = top_10_eviction_rate_last_year\n",
    "    \n",
    "    \n",
    "data['top_10_percent_in_any_next_3_years'] = top_10_eviction_rate_in_any_next_3_years_column\n",
    "\n",
    "label ='top_10_percent_in_any_next_3_years'\n",
    "\n",
    "#Feature\n",
    "# data['top_10_percent_last_year']=top_10_eviction_rate_last_year_column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging. In case we want to add 1 column for each future year with top 10%\n",
    "\n",
    "      # eviction_rate_in_1_year = np.zeros(len(data))\n",
    "# cutoff_in_1_year = np.zeros(len(data))\n",
    "\n",
    "      \n",
    "#       if(top_10_eviction_rate_in_i_years==1):\n",
    "#         print(row['GEOID'])\n",
    "#         print(row['year'])\n",
    "#         print(date_in_i_years)\n",
    "#         print(eviction_rate_in_i_years)\n",
    "#         print(cutoff_10_percent[date_in_i_years])\n",
    "#         print(top_10_eviction_rate_in_i_years)     \n",
    "\n",
    "#       if(i==1):\n",
    "#         eviction_rate_in_1_year[index]=eviction_rate_in_i_years\n",
    "#         cutoff_in_1_year[index]=cutoff_10_percent[date_in_i_years]\n",
    "\n",
    "# data['eviction_rate_in_1_year']='eviction_rate_in_1_year'\n",
    "# data['cutoff_in_1_year']='eviction_rate_in_1_year'      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create temporal train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "#Create sets of train and test data, based on different split thresholds\n",
    "#The split thresholds corresponds to the starting date of the testing data\n",
    "\n",
    "#Splits according to https://docs.google.com/spreadsheets/d/1ipqsgThz7hdXXyyNpTuqa4J1inc088lop7lhFsAQ_r0/edit#gid=0\n",
    "split_thresholds = [pd.Timestamp(i,1,1) for i in range (2004, 2014)]\n",
    "\n",
    "#Indicating which is the column to be used for splitting training and test daata\n",
    "date_column='year'\n",
    "\n",
    "#Amount of data used for test set\n",
    "test_window = relativedelta(years=4)\n",
    "\n",
    "#Gap needed between training and test set\n",
    "prediction_horizon = relativedelta(years=3)\n",
    "\n",
    "#Generate train and test sets\n",
    "train_test_sets= pipeline.create_temp_validation_train_and_testing_sets(\n",
    "  data,\n",
    "  date_column,\n",
    "  label,\n",
    "  split_thresholds,\n",
    "  test_window,\n",
    "  prediction_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID\n",
      "year\n",
      "name\n",
      "parent-location\n",
      "population\n",
      "poverty-rate\n",
      "renter-occupied-households\n",
      "pct-renter-occupied\n",
      "median-gross-rent\n",
      "median-household-income\n",
      "median-property-value\n",
      "rent-burden\n",
      "pct-white\n",
      "pct-af-am\n",
      "pct-hispanic\n",
      "pct-am-ind\n",
      "pct-asian\n",
      "pct-nh-pi\n",
      "pct-multiple\n",
      "pct-other\n",
      "eviction-filings\n",
      "evictions\n",
      "eviction-rate\n",
      "eviction-filing-rate\n",
      "low-flag\n",
      "imputed\n",
      "subbed\n"
     ]
    }
   ],
   "source": [
    "for c in train_test_sets[5]['x_train'].columns:\n",
    "  print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "#Impute data on continuous columns for each training and test set\n",
    "\n",
    "#--->PENDING\n",
    "#In the meantime, imputing all float columns with mean\n",
    "  \n",
    "float_columns = [column for column in data.columns if data[column].dtype=='float']\n",
    "\n",
    "#Do not consider GEOID column nor top_10_percent_in_any_next_3_years\n",
    "float_columns.remove('name')\n",
    "float_columns.remove('top_10_percent_in_any_next_3_years')\n",
    "\n",
    "\n",
    "for train_test_set in train_test_sets:\n",
    "  train_data = train_test_set['x_train']\n",
    "  test_data = train_test_set['x_test']\n",
    "\n",
    "  #fill na values with mean\n",
    "  pipeline.impute_data(train_data, float_columns)\n",
    "  pipeline.impute_data(test_data, float_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feature_generation as fg\n",
    "\n",
    "importlib.reload(pipeline)\n",
    "importlib.reload(fg)\n",
    "\n",
    "#We will have to generate features independently for each different train/test set\n",
    "for train_test_set in train_test_sets:\n",
    "\n",
    "  train_features = fg.create_features(train_test_set['x_train'])\n",
    "  test_features = fg.create_features(train_test_set['x_test']) \n",
    "  \n",
    "  #Alternative for just working with default features\n",
    "  #train_features, test_features = pipeline.create_features(train_test_set)\n",
    "  \n",
    "  #Replace raw data in train_test_set with features generated\n",
    "  train_test_set['x_train'] = train_features\n",
    "  train_test_set['x_test'] = test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Clasifiers and parameters generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "#We define the specific models we want to run\n",
    "models_to_run=['Baseline','DT','LR','RF','NB','BA','AB','GB']#,ET,KNN,SVM'\n",
    "\n",
    "#Get all posible models and their different sets of parameters\n",
    "models, parameters_grid = pipeline.get_models_and_parameters('small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over models and different training/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(len(train_test_sets[0]['x_test']))\n",
    "# # print(len(train_test_sets[0]['y_test']))\n",
    "# train_test_sets[0]['y_test'].isnull().any()\n",
    "# # df.isnull().values.any()\n",
    "\n",
    "# # for column in train_test_sets[5]['x_test'].columns:\n",
    "# #   print(column)\n",
    "# #   \n",
    "# train_test_sets[5]['x_train'].to_csv(\"columns.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-12 12:16:39.391763: Running Baseline with params: {} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:16:39.977862: Running DT with params: {'criterion': 'gini', 'max_depth': 2, 'min_samples_split': 2} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:16:40.332184: Running DT with params: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:16:41.045705: Running DT with params: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:16:41.477932: Running LR with params: {'C': 0.1, 'penalty': 'l1'} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:16:42.383059: Running LR with params: {'C': 0.1, 'penalty': 'l2'} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:16:43.202484: Running LR with params: {'C': 10, 'penalty': 'l1'} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:04.143561: Running LR with params: {'C': 10, 'penalty': 'l2'} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:04.580269: Running RF with params: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 10, 'n_jobs': -1} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:05.088772: Running RF with params: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100, 'n_jobs': -1} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:05.695695: Running RF with params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 10, 'n_jobs': -1} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:06.204398: Running RF with params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100, 'n_jobs': -1} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:06.882795: Running NB with params: {} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:07.205613: Running BA with params: {'n_estimators': 10} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:08.615875: Running BA with params: {'n_estimators': 100} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:19.036424: Running AB with params: {'algorithm': 'SAMME', 'n_estimators': 10} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:19.555171: Running AB with params: {'algorithm': 'SAMME', 'n_estimators': 100} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:21.083241: Running GB with params: {'learning_rate': 0.001, 'n_estimators': 10} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:21.638046: Running GB with params: {'learning_rate': 0.001, 'n_estimators': 100} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:23.187531: Running GB with params: {'learning_rate': 0.1, 'n_estimators': 10} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:23.911388: Running GB with params: {'learning_rate': 0.1, 'n_estimators': 100} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:25.774770: Running ET with params: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 100} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:26.910787: Running ET with params: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 100} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:27.909125: Running ET with params: {'criterion': 'gini', 'max_depth': 50, 'n_estimators': 100} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:28.633090: Running ET with params: {'criterion': 'gini', 'max_depth': 50, 'n_estimators': 100} on train/test set 2004-01-01 00:00:00\n",
      "2019-06-12 12:17:29.515658: Running Baseline with params: {} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:17:29.999225: Running DT with params: {'criterion': 'gini', 'max_depth': 2, 'min_samples_split': 2} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:17:30.516205: Running DT with params: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:17:31.101476: Running DT with params: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:17:31.601672: Running LR with params: {'C': 0.1, 'penalty': 'l1'} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:17:40.738373: Running LR with params: {'C': 0.1, 'penalty': 'l2'} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:17:41.927445: Running LR with params: {'C': 10, 'penalty': 'l1'} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:18:32.320786: Running LR with params: {'C': 10, 'penalty': 'l2'} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:18:33.191677: Running RF with params: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 10, 'n_jobs': -1} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:18:34.288619: Running RF with params: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100, 'n_jobs': -1} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:18:35.269753: Running RF with params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 10, 'n_jobs': -1} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:18:36.264436: Running RF with params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100, 'n_jobs': -1} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:18:37.365746: Running NB with params: {} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:18:38.083040: Running BA with params: {'n_estimators': 10} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:18:40.048286: Running BA with params: {'n_estimators': 100} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:18:54.396769: Running AB with params: {'algorithm': 'SAMME', 'n_estimators': 10} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:18:55.068277: Running AB with params: {'algorithm': 'SAMME', 'n_estimators': 100} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:18:56.681593: Running GB with params: {'learning_rate': 0.001, 'n_estimators': 10} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:18:57.164453: Running GB with params: {'learning_rate': 0.001, 'n_estimators': 100} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:18:58.890661: Running GB with params: {'learning_rate': 0.1, 'n_estimators': 10} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:18:59.581693: Running GB with params: {'learning_rate': 0.1, 'n_estimators': 100} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:19:01.537332: Running ET with params: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 100} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:19:02.385029: Running ET with params: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 100} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:19:03.279182: Running ET with params: {'criterion': 'gini', 'max_depth': 50, 'n_estimators': 100} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:19:04.206128: Running ET with params: {'criterion': 'gini', 'max_depth': 50, 'n_estimators': 100} on train/test set 2005-01-01 00:00:00\n",
      "2019-06-12 12:19:05.056086: Running Baseline with params: {} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:19:05.392254: Running DT with params: {'criterion': 'gini', 'max_depth': 2, 'min_samples_split': 2} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:19:05.926324: Running DT with params: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:19:06.900447: Running DT with params: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:19:07.384339: Running LR with params: {'C': 0.1, 'penalty': 'l1'} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:19:12.354210: Running LR with params: {'C': 0.1, 'penalty': 'l2'} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:19:13.588607: Running LR with params: {'C': 10, 'penalty': 'l1'} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:20:30.089457: Running LR with params: {'C': 10, 'penalty': 'l2'} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:20:30.909238: Running RF with params: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 10, 'n_jobs': -1} on train/test set 2006-01-01 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-12 12:20:31.530818: Running RF with params: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100, 'n_jobs': -1} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:20:32.372730: Running RF with params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 10, 'n_jobs': -1} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:20:33.083332: Running RF with params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100, 'n_jobs': -1} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:20:34.102275: Running NB with params: {} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:20:34.535118: Running BA with params: {'n_estimators': 10} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:20:36.478362: Running BA with params: {'n_estimators': 100} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:20:54.649982: Running AB with params: {'algorithm': 'SAMME', 'n_estimators': 10} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:20:55.149694: Running AB with params: {'algorithm': 'SAMME', 'n_estimators': 100} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:20:56.936493: Running GB with params: {'learning_rate': 0.001, 'n_estimators': 10} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:20:57.531192: Running GB with params: {'learning_rate': 0.001, 'n_estimators': 100} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:21:00.174733: Running GB with params: {'learning_rate': 0.1, 'n_estimators': 10} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:21:00.906772: Running GB with params: {'learning_rate': 0.1, 'n_estimators': 100} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:21:03.783704: Running ET with params: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 100} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:21:05.054327: Running ET with params: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 100} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:21:06.113907: Running ET with params: {'criterion': 'gini', 'max_depth': 50, 'n_estimators': 100} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:21:06.917736: Running ET with params: {'criterion': 'gini', 'max_depth': 50, 'n_estimators': 100} on train/test set 2006-01-01 00:00:00\n",
      "2019-06-12 12:21:07.745930: Running Baseline with params: {} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:21:08.337061: Running DT with params: {'criterion': 'gini', 'max_depth': 2, 'min_samples_split': 2} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:21:08.710461: Running DT with params: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:21:09.227010: Running DT with params: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:21:09.735593: Running LR with params: {'C': 0.1, 'penalty': 'l1'} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:21:41.546265: Running LR with params: {'C': 0.1, 'penalty': 'l2'} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:21:43.248174: Running LR with params: {'C': 10, 'penalty': 'l1'} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:23:34.761047: Running LR with params: {'C': 10, 'penalty': 'l2'} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:23:36.033506: Running RF with params: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 10, 'n_jobs': -1} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:23:36.656858: Running RF with params: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100, 'n_jobs': -1} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:23:37.663288: Running RF with params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 10, 'n_jobs': -1} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:23:38.240459: Running RF with params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100, 'n_jobs': -1} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:23:39.368594: Running NB with params: {} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:23:39.867734: Running BA with params: {'n_estimators': 10} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:23:42.587959: Running BA with params: {'n_estimators': 100} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:24:10.391348: Running AB with params: {'algorithm': 'SAMME', 'n_estimators': 10} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:24:10.969814: Running AB with params: {'algorithm': 'SAMME', 'n_estimators': 100} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:24:13.372311: Running GB with params: {'learning_rate': 0.001, 'n_estimators': 10} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:24:14.073944: Running GB with params: {'learning_rate': 0.001, 'n_estimators': 100} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:24:18.304848: Running GB with params: {'learning_rate': 0.1, 'n_estimators': 10} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:24:19.508261: Running GB with params: {'learning_rate': 0.1, 'n_estimators': 100} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:24:24.040135: Running ET with params: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 100} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:24:24.832473: Running ET with params: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 100} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:24:25.864930: Running ET with params: {'criterion': 'gini', 'max_depth': 50, 'n_estimators': 100} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:24:27.819057: Running ET with params: {'criterion': 'gini', 'max_depth': 50, 'n_estimators': 100} on train/test set 2007-01-01 00:00:00\n",
      "2019-06-12 12:24:29.336206: Running Baseline with params: {} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:24:29.806628: Running DT with params: {'criterion': 'gini', 'max_depth': 2, 'min_samples_split': 2} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:24:30.281037: Running DT with params: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:24:31.054162: Running DT with params: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:24:31.755318: Running LR with params: {'C': 0.1, 'penalty': 'l1'} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:25:25.271746: Running LR with params: {'C': 0.1, 'penalty': 'l2'} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:25:26.918964: Running LR with params: {'C': 10, 'penalty': 'l1'} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:28:17.114766: Running LR with params: {'C': 10, 'penalty': 'l2'} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:28:18.224139: Running RF with params: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 10, 'n_jobs': -1} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:28:18.855771: Running RF with params: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100, 'n_jobs': -1} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:28:19.852219: Running RF with params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 10, 'n_jobs': -1} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:28:20.714067: Running RF with params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100, 'n_jobs': -1} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:28:22.165793: Running NB with params: {} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:28:22.656200: Running BA with params: {'n_estimators': 10} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:28:25.468231: Running BA with params: {'n_estimators': 100} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:28:48.887357: Running AB with params: {'algorithm': 'SAMME', 'n_estimators': 10} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:28:49.544590: Running AB with params: {'algorithm': 'SAMME', 'n_estimators': 100} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:28:52.298846: Running GB with params: {'learning_rate': 0.001, 'n_estimators': 10} on train/test set 2008-01-01 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-12 12:28:52.964157: Running GB with params: {'learning_rate': 0.001, 'n_estimators': 100} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:28:56.398076: Running GB with params: {'learning_rate': 0.1, 'n_estimators': 10} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:28:57.099110: Running GB with params: {'learning_rate': 0.1, 'n_estimators': 100} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:29:00.259359: Running ET with params: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 100} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:29:00.907413: Running ET with params: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 100} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:29:01.750266: Running ET with params: {'criterion': 'gini', 'max_depth': 50, 'n_estimators': 100} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:29:02.771695: Running ET with params: {'criterion': 'gini', 'max_depth': 50, 'n_estimators': 100} on train/test set 2008-01-01 00:00:00\n",
      "2019-06-12 12:29:03.884579: Running Baseline with params: {} on train/test set 2009-01-01 00:00:00\n",
      "2019-06-12 12:29:04.251771: Running DT with params: {'criterion': 'gini', 'max_depth': 2, 'min_samples_split': 2} on train/test set 2009-01-01 00:00:00\n",
      "2019-06-12 12:29:04.671443: Running DT with params: {'criterion': 'gini', 'max_depth': 50, 'min_samples_split': 2} on train/test set 2009-01-01 00:00:00\n",
      "2019-06-12 12:29:05.330700: Running DT with params: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 2} on train/test set 2009-01-01 00:00:00\n",
      "2019-06-12 12:29:06.100031: Running LR with params: {'C': 0.1, 'penalty': 'l1'} on train/test set 2009-01-01 00:00:00\n",
      "2019-06-12 12:29:10.608045: Running LR with params: {'C': 0.1, 'penalty': 'l2'} on train/test set 2009-01-01 00:00:00\n",
      "2019-06-12 12:29:12.076748: Running LR with params: {'C': 10, 'penalty': 'l1'} on train/test set 2009-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pipeline)\n",
    "importlib.reload(loop)\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "results = loop.iterate_over_models_and_training_test_sets(models_to_run, models, parameters_grid, train_test_sets)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe best models for each train/test set, for different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "#Lets obtain the best model for each train/test set, for each metric\n",
    "metrics_to_display = ['p_at_5','p_at_10', 'auc-roc']\n",
    "\n",
    "best_models_per_metric = {}\n",
    "\n",
    "for metric in metrics_to_display:\n",
    "    #indices of rows that have max value in specific metric for each train/test set\n",
    "    idx = results.groupby(['test_set_start_date'])[metric].transform(max) == results[metric]\n",
    "\n",
    "    #save table of best models at the specific metric\n",
    "    best_models_per_metric[metric] = results[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best models for Precision at 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_models_per_metric['p_at_5'].iloc[:, [0,2,3,4,11,12,13]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best models for Precision at 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_models_per_metric['p_at_10'].iloc[:, [0,2,3,4,14,15,16]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best models for AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_per_metric['auc-roc'].iloc[:, [0,2,3,4,26]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of all model types performance at different train/test sets, for the different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "best_models_per_metric = {}\n",
    "\n",
    "for metric in metrics_to_display:\n",
    "    #For each model, find the set of parameters that work the best in each train/test set\n",
    "    best_models = pipeline.get_best_models_of_each_type_for_each_train_test_set(models_to_run,results,'test_set_start_date', metric)\n",
    "    pipeline.plot_models_in_time(models_to_run, best_models, metric)\n",
    "    best_models_per_metric[metric]=best_models\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
